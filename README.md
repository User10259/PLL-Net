# Continuous Global Power Line Localization via Local Polar-Coordinate Feature Learning


## ğŸš€ Network Architecture

<img src="architecture.jpg">

PLD-Net consists of:
- Grid-wise Polar Representation â€” Divides the image into grid cells and encodes local line geometry in polar coordinates.
- ConvNeXt-E Backbone â€” Enlarges the effective receptive field for structural line features.
- Feature Enhancement Modules (WGA & UND) â€” Strengthen thin-line responses and suppress background noise.
- Prediction Head â€” Outputs classification scores and polar regression parameters for each grid cell.
- Hungarian Matching â€” Provides one-to-one supervision between predictions and ground truth during training.
- Global Polar Reconstruction â€” Converts cell-wise predictions into global line segments during inference.
- Line Integration â€” Merges and stitches segments to produce continuous power lines.

---

## ğŸ“Š Quantitative Comparison with State-of-the-Art

The following table shows our comparison on the **PLDU dataset** and **TTPLA dataset**.

### **Results on PLDU & TTPLA**

| Dataset | Method | UNet | Swin-Unet | DDRNet | MADG-Net | MiT-Unet | SegFormer | **PLD-Net** |
|--------|--------|------|-----------|--------|-----------|-----------|-----------|-------------|
| **PLDU** | F1  | 87.51 | 84.95 | 87.77 | 88.80 | 88.99 | 86.88 | **89.43** |
|         | IoU | 77.64 | 73.12 | 78.12 | 79.55 | 79.73 | 76.12 | **82.04** |
| **TTPLA** | F1 | 77.95 | 77.75 | 77.93 | 78.43 | 82.50 | 72.40 | **82.74** |
|          | IoU | 61.33 | 61.36 | 61.48 | 63.66 | 67.98 | 53.42 | **69.27** |
| **Params (M)** | â€” | 31.04 | 213.96 | 20.17 | 31.22 | 19.27 | 3.71 | 49.77 |
| **FLOPs (G)**  | â€” | 218.95 | 180.32 | 17.97 | 39.76 | 73.25 | 6.76 | 27.44 |

---

## ğŸ–¼ï¸ Visualization Results

<img src="result-pldu.jpg" height="520">

Note: Visualization of segmented results on PLDU dataset.

<img src="result-ttpla.jpg" height="520">

Note: Visualization of segmented results on TTPLA dataset.

---

## ğŸ“ Project Structure
We only provide partial code for testing purposes.
The available files and structure are as follows:
```text
PLD-Net/
â”‚
â”œâ”€â”€ checkpoints/
â”‚ â”œâ”€â”€ bestpldu.pth
â”‚ â””â”€â”€ bestttpla.pth
â”‚
â”œâ”€â”€ datasets/
â”‚ â”œâ”€â”€ pldu/
â”‚ â””â”€â”€ ttpla/
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ PLDNet.py
â”‚ â”œâ”€â”€ UND.py
â”‚ â””â”€â”€ WaveletAttention.py
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ postprocessPLDU.py 
â”‚ â”œâ”€â”€ postprocessTTPLA.py.
â”‚ â””â”€â”€ logger.py

â”œâ”€â”€ demo.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
Note: We place the best-performing training weights in the GitHub Releases section instead of the main code directory. To maintain anonymity during the review process, 
the weights will be made available via GitHub Releases immediately upon paper acceptance.

## ğŸ“¦ Requirements

Please install the following packages:
- torch
- torchvision
- timm
- numpy
- opencv-python
- Pillow
- shapely
- albumentations
- matplotlib
- tqdm
- thop
- scipy
- scikit-image

You may create a virtual environment:
```bash
conda create -n pldnet python=3.8
conda activate pldnet
pip install -r requirements.txt
```


## â–¶ï¸ Test (Demo Inference)

We provide the best-performing training weights for PLDU and TTPLA.
Use the following command to run inference on a single image:
```bash
python demo.py --dataset pldu --img datasets/pldu/1.jpg
```
or for TTPLA:
```bash
python demo.py --dataset ttpla --img datasets/ttpla/08_1830.jpg
```
The results will be saved under outputs. Modify the image path as needed to test your own images.
